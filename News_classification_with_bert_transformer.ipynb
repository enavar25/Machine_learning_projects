{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnr7IZ8/6Tfvqysub69/dS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/enavar25/Machine_learning_projects/blob/main/News_classification_with_bert_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project:** News classification detection with a transformer network (BERT)"
      ],
      "metadata": {
        "id": "hbdNfKkLXw31"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here I will use a bert model to classify if text from a news article is either fake or true news. The dataset that was used was the ISOT Fake News Dataset, which contains two types of articles fake and real News. This dataset was collected from real world sources.**\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "GP3odmj3YXil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Below, I will import all necessary functions and libraries.\n",
        "\n"
      ],
      "metadata": {
        "id": "oR-0MRZJWGIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet gdown==4.5.4 --no-cache-dir # I need this to import large files from google drive\n",
        "\n",
        "!pip install -q -U \"tensorflow-text==2.11.*\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "\n",
        "import string\n",
        "from sklearn.feature_extraction import text\n",
        "stopwords = text.ENGLISH_STOP_WORDS    # need to clean up tokenzation process \n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import layers, Model, metrics\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "##################################\n",
        "!gdown 1lEK4q5rbE2WWrwATUgnsFG5LI8TvTE9Y\n",
        "!unzip -qq real_or_fake_news.zip;"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8rbd-FmUP1T",
        "outputId": "ab62ab51-2e7c-4684-e117-5f4de25c4bef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading...\n",
            "From: https://drive.google.com/uc?id=1lEK4q5rbE2WWrwATUgnsFG5LI8TvTE9Y\n",
            "To: /content/real_or_fake_news.zip\n",
            "100% 43.0M/43.0M [00:00<00:00, 144MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, I make two panda dataframes and assign the label 1 to real news and 0 to fake news. I also check for null and duplicate entries and drop duplicate entries since there are no null entries. I also combine the true and fake dataframes into one dataframe. We are also going to preprocess the text by removing punctuation and stopwords (unwanted tokens)."
      ],
      "metadata": {
        "id": "3KVsFHqfYYvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "true = pd.read_csv(\"real_news.csv\")\n",
        "fake = pd.read_csv(\"fake_news.csv\")\n",
        "\n",
        "true['category'] = 1\n",
        "fake['category'] = 0\n",
        "#####################################\n",
        "fake.isnull().sum()\n",
        "fake.duplicated().sum()\n",
        "fake = fake.drop_duplicates()\n",
        "true.isnull().sum()\n",
        "true.duplicated().sum()\n",
        "true= true.drop_duplicates()\n",
        "##########################################\n",
        "true.reset_index(drop=True, inplace=True)\n",
        "fake.reset_index(drop=True, inplace=True)\n",
        "\n",
        "df = pd.concat([true,fake])\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "\n",
        "def remove_punctuations(text):\n",
        "    for punctuation in string.punctuation:\n",
        "        text = text.replace(punctuation, '')\n",
        "    return text\n",
        "\n",
        "# Use pandas to apply the remove_punctuations function to the 'text' column\n",
        "df['processed_text'] = df['text'].apply(remove_punctuations)\n",
        "# Covert the processed text to lowercase\n",
        "df['processed_text'] = df['processed_text'].apply(str.lower)\n",
        "# Remove all the stopwords from the processed text\n",
        "df['processed_text'] = df['processed_text'].apply(\n",
        "    lambda x: ' '.join([word for word in x.split() if word not in (stopwords)])\n",
        "    )\n",
        "#####\n"
      ],
      "metadata": {
        "id": "Sm9o1zJeWY8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I create our outputs and inputs with our preprocessed text and split our dataframe into train and test datasets below"
      ],
      "metadata": {
        "id": "6uzQWyGWYa0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = df['processed_text']\n",
        "output = df['category']\n",
        "X_train, X_test, y_train, y_test = train_test_split(inputs,output)"
      ],
      "metadata": {
        "id": "m8GrcKjNWt8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we download the bert model with its  tokenization and embedding/vectorization layers. Note we already have done some tokenization for the dataframe but this layer has more preprocessing steps that I have not done yet."
      ],
      "metadata": {
        "id": "TD3wxc6UbYi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_text as text\n",
        "bertPreprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
        "\n",
        "bertEncode = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")\n"
      ],
      "metadata": {
        "id": "sURFR3EdbdNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we start building the bert model by initializing the tokenization layer and the encoding/embedding layer"
      ],
      "metadata": {
        "id": "N2ssL4e3cYsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputText = layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "preprocessedText = bertPreprocess(inputText)\n",
        "outputs = bertEncode(preprocessedText)"
      ],
      "metadata": {
        "id": "jNjb69PQKRGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we start initializing our neural network layers with a dropout layer to prevent overfitting and a dense layer which will be our output layer. The dense output layer will use a sigmoid function to output numbers between 0 and 1 since this is a binary classification task"
      ],
      "metadata": {
        "id": "lVSI3t3GeHaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b = layers.Dropout(0.1, name=\"dropout\")(outputs['pooled_output'])\n",
        "b = layers.Dense(1, activation='sigmoid', name=\"output\")(b)"
      ],
      "metadata": {
        "id": "2y5NjPSKKmdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we construct the model with the input and output layer and see a summary of the model to see the layers of our model\n"
      ],
      "metadata": {
        "id": "K1RNsyNWfKte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(inputs=[inputText], outputs = [b])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Peod3q7AKucH",
        "outputId": "1f17b8ab-f534-4bf8-a23f-638d531691de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " text (InputLayer)              [(None,)]            0           []                               \n",
            "                                                                                                  \n",
            " keras_layer_2 (KerasLayer)     {'input_word_ids':   0           ['text[0][0]']                   \n",
            "                                (None, 128),                                                      \n",
            "                                 'input_mask': (Non                                               \n",
            "                                e, 128),                                                          \n",
            "                                 'input_type_ids':                                                \n",
            "                                (None, 128)}                                                      \n",
            "                                                                                                  \n",
            " keras_layer_3 (KerasLayer)     {'encoder_outputs':  109482241   ['keras_layer_2[0][0]',          \n",
            "                                 [(None, 128, 768),               'keras_layer_2[0][1]',          \n",
            "                                 (None, 128, 768),                'keras_layer_2[0][2]']          \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768)],                                               \n",
            "                                 'pooled_output': (                                               \n",
            "                                None, 768),                                                       \n",
            "                                 'default': (None,                                                \n",
            "                                768),                                                             \n",
            "                                 'sequence_output':                                               \n",
            "                                 (None, 128, 768)}                                                \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 768)          0           ['keras_layer_3[0][13]']         \n",
            "                                                                                                  \n",
            " output (Dense)                 (None, 1)            769         ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,483,010\n",
            "Trainable params: 769\n",
            "Non-trainable params: 109,482,241\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we start putting in the parameters to compile our model. To optimize our model we use adam, and we assign binary_crossentropy to our loss function since this is a binary classification task. The only metrics that we will use is accuracy and precision."
      ],
      "metadata": {
        "id": "De5Sfuh2hl3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Metrics = [\n",
        "      metrics.BinaryAccuracy(name='accuracy'),\n",
        "      metrics.Precision(name='precision')\n",
        "]\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=Metrics)"
      ],
      "metadata": {
        "id": "LlNevQlsLdOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we train our model"
      ],
      "metadata": {
        "id": "AtjULpFUimYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzD0muhsLiAf",
        "outputId": "a7e504b5-e8bd-4bdb-dd8f-5279ae41edbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1048/1048 [==============================] - 376s 351ms/step - loss: 0.3888 - accuracy: 0.8574 - precision: 0.8605\n",
            "Epoch 2/5\n",
            "1048/1048 [==============================] - 366s 349ms/step - loss: 0.2524 - accuracy: 0.9085 - precision: 0.9027\n",
            "Epoch 3/5\n",
            "1048/1048 [==============================] - 365s 349ms/step - loss: 0.2255 - accuracy: 0.9170 - precision: 0.9122\n",
            "Epoch 4/5\n",
            "1048/1048 [==============================] - 365s 348ms/step - loss: 0.2113 - accuracy: 0.9209 - precision: 0.9158\n",
            "Epoch 5/5\n",
            "1048/1048 [==============================] - 364s 348ms/step - loss: 0.2020 - accuracy: 0.9208 - precision: 0.9148\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f45f5b71880>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After 5 epochs, we see that the model has an accuracy of 92.08 percent and a precision of 91.48 percent"
      ],
      "metadata": {
        "id": "5yaYRf_HkME_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now use our model to predict and classify out text. Below in the small test list we can type any string text in the elements and use our model to predict if the text is fake or true. If the prediction is more than or equal to .50, then it is fake.Else it is true. "
      ],
      "metadata": {
        "id": "3uhul7igiu3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "small_test =['put your text element here','you can type more than one element']\n",
        "model.predict(small_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_xQjVEe69xm",
        "outputId": "ef2169e8-9a7a-47a6-bd87-441d9b52da62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 43ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00814331],\n",
              "       [0.14144714]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}