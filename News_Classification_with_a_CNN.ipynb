{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpbNxZ/DgKAsj4r/adv5zB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/enavar25/Machine_learning_projects/blob/main/News_Classification_with_a_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project:** News classification detection with a CNN "
      ],
      "metadata": {
        "id": "kmBMVb-h5yaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here I will build a simple CNN using keras to classify if a text is either fake or true news. The dataset that was used was the ISOT Fake News Dataset, which contains two types of articles fake and real News. This dataset was collected from real world sources.**\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "r3s7mLboYPYZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Below, I will import all necessary functions and libraries.\n",
        "\n"
      ],
      "metadata": {
        "id": "oR-0MRZJWGIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet gdown==4.5.4 --no-cache-dir # I need this to import large files from google drive\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import string\n",
        "from sklearn.feature_extraction import text\n",
        "stopwords = text.ENGLISH_STOP_WORDS    # need to clean up tokenzation process \n",
        "\n",
        "\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import *\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "##################################\n",
        "!gdown 1lEK4q5rbE2WWrwATUgnsFG5LI8TvTE9Y\n",
        "!unzip -qq real_or_fake_news.zip;"
      ],
      "metadata": {
        "id": "zmkP5LqLYVYo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e637687-426b-4666-e30b-9a11d6695e77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1lEK4q5rbE2WWrwATUgnsFG5LI8TvTE9Y\n",
            "To: /content/real_or_fake_news.zip\n",
            "100% 43.0M/43.0M [00:00<00:00, 67.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, I make two panda dataframes and assign the label 1 to real news and 0 to fake news. I also check for null and duplicate entries and drop duplicate entries since there are no null entries."
      ],
      "metadata": {
        "id": "3KVsFHqfYYvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "true = pd.read_csv(\"real_news.csv\")\n",
        "fake = pd.read_csv(\"fake_news.csv\")\n",
        "\n",
        "true['category'] = 1\n",
        "fake['category'] = 0\n",
        "#####################################\n",
        "fake.isnull().sum()\n",
        "fake.duplicated().sum()\n",
        "fake = fake.drop_duplicates()\n",
        "true.isnull().sum()\n",
        "true.duplicated().sum()\n",
        "true= true.drop_duplicates()"
      ],
      "metadata": {
        "id": "4ntzahh7YaH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I combine the true and fake dataframes into one dataframe. We are also going to preprocess the text by removing punctuation and stopwords (unwanted tokens).\n",
        "\n"
      ],
      "metadata": {
        "id": "6NDP1W2UYalQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the true and fake news datasets into one dataframe.\n",
        "df = pd.concat([true,fake]) \n",
        "\n",
        "# Define a function to remove punctuation.\n",
        "def remove_punctuations(text):\n",
        "    for punctuation in string.punctuation:\n",
        "        text = text.replace(punctuation, '')\n",
        "    return text\n",
        "\n",
        "# Use pandas to apply the remove_punctuations function to the 'text' column\n",
        "df['processed_text'] = df['text'].apply(remove_punctuations)\n",
        "# Covert the processed text to lowercase\n",
        "df['processed_text'] = df['processed_text'].apply(str.lower)\n",
        "# Remove all the stopwords from the processed text\n",
        "df['processed_text'] = df['processed_text'].apply(\n",
        "    lambda x: ' '.join([word for word in x.split() if word not in (stopwords)])\n",
        "    )"
      ],
      "metadata": {
        "id": "KIHPmN3eYatX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I create our outputs and inputs with our preprocessed text and split our dataframe into train and test datasets"
      ],
      "metadata": {
        "id": "6uzQWyGWYa0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "inputs = df['processed_text']\n",
        "output = df['category']\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    inputs, \n",
        "    output, \n",
        "    shuffle=True,\n",
        "    test_size=0.2, \n",
        "    random_state=42\n",
        " )"
      ],
      "metadata": {
        "id": "gjYR5bFnYa7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "creating a vectorization layer with a maximum of 5000 word tokens and adapting it to the training data. TextVectorization converts a batch of strings into tokens and converts each token into a vector of either dense floats or integers."
      ],
      "metadata": {
        "id": "bfy-oJfZYbBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer =  TextVectorization(max_tokens=5000,output_mode= 'int', output_sequence_length=100) #tokenize and vectorize \n",
        "vectorize_layer.adapt(x_train)"
      ],
      "metadata": {
        "id": "mDznfTxNYbHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing our neural network and adding dense, convolutional, embedding, dense, and max pooling layers. "
      ],
      "metadata": {
        "id": "B4pfUC1BYbOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Input(shape=(1,), dtype=tf.string))\n",
        "model.add(vectorize_layer)\n",
        "model.add(Embedding(input_dim = 5000,output_dim = 100,input_length = 100)) # the embedding layer catches relationships and meaning between words using linear algebra\n",
        "          \n",
        "model.add(Conv1D(filters = 128, kernel_size= 5, activation ='relu'))\n",
        "model.add(MaxPool1D(pool_size=2))\n",
        "model.add(Dense(256, activation = 'relu'))\n",
        "model.add(Dense(1,activation = 'sigmoid'))"
      ],
      "metadata": {
        "id": "VlZzgqyFYbUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a summary of the model to see our total parameters "
      ],
      "metadata": {
        "id": "U8-OuZqZUnEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "atyGATp6YkZZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "321e7bbe-f860-467d-c7e1-cf800159f20b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization (TextVec  (None, 100)              0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 100, 100)          500000    \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 96, 128)           64128     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 48, 128)          0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dense (Dense)               (None, 48, 256)           33024     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 48, 1)             257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 597,409\n",
            "Trainable params: 597,409\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compiling and training the model \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4g1zDRObYkv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "opt = Adam(learning_rate= 0.01)\n",
        "model.compile(optimizer = opt, loss= 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs = 5, batch_size = 256);"
      ],
      "metadata": {
        "id": "XR20RxMXYlKg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6062cdb2-52b7-4122-f99b-48825cd876d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "140/140 [==============================] - 50s 352ms/step - loss: 0.4597 - accuracy: 0.7643\n",
            "Epoch 2/5\n",
            "140/140 [==============================] - 46s 327ms/step - loss: 0.4228 - accuracy: 0.7853\n",
            "Epoch 3/5\n",
            "140/140 [==============================] - 47s 337ms/step - loss: 0.4051 - accuracy: 0.7947\n",
            "Epoch 4/5\n",
            "140/140 [==============================] - 47s 333ms/step - loss: 0.3879 - accuracy: 0.8031\n",
            "Epoch 5/5\n",
            "140/140 [==============================] - 47s 333ms/step - loss: 0.3708 - accuracy: 0.8129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the model's accuracy for both the train and test sets."
      ],
      "metadata": {
        "id": "o_N4I7jdH5wd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_train,y_train)\n",
        "model.evaluate(x_test,y_test)"
      ],
      "metadata": {
        "id": "J_PkQzTuH_jj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5625377-7918-4717-d80d-eae3af3e9bef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1118/1118 [==============================] - 19s 17ms/step - loss: 0.3401 - accuracy: 0.8304\n",
            "280/280 [==============================] - 4s 15ms/step - loss: 0.4115 - accuracy: 0.7992\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.41151779890060425, 0.7992444634437561]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall the model is around 80 percent accurate, which is not that bad. In the next project I will see if I can get better accuracy in a similar text classification task with an attention mechanism model (Transformer)."
      ],
      "metadata": {
        "id": "GzMsHj9aAN6P"
      }
    }
  ]
}